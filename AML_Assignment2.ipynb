{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 2 - MiniPacman"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random\n",
    "import time\n",
    "import pyglet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from mini_pacman import PacmanGame\n",
    "\n",
    "with open('test_params.json', 'r') as file:\n",
    "    read_params = json.load(file)\n",
    "game_params = read_params['params']\n",
    "env = PacmanGame(**game_params)\n",
    "\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|   Action Code | Move       |\n",
      "|---------------+------------|\n",
      "|             1 | Down-Left  |\n",
      "|             2 | Down       |\n",
      "|             3 | Down-Right |\n",
      "|             4 | Left       |\n",
      "|             5 | No Move    |\n",
      "|             6 | Right      |\n",
      "|             7 | Up-Left    |\n",
      "|             8 | Up         |\n",
      "|             9 | Up-Right   |\n"
     ]
    }
   ],
   "source": [
    "from tabulate import tabulate\n",
    "print(tabulate([[1,'Down-Left'], \\\n",
    "                [2,'Down'], \\\n",
    "                [3,'Down-Right'], \\\n",
    "                [4,'Left'], \\\n",
    "                [5,'No Move'], \\\n",
    "                [6,'Right'], \\\n",
    "                [7,'Up-Left'], \\\n",
    "                [8,'Up'], \\\n",
    "                [9,'Up-Right']], \\\n",
    "               headers = ['Action Code','Move'], \\\n",
    "              tablefmt='orgtbl'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs = env.make_action(5)\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'reward': 0,\n",
       " 'total_score': 0,\n",
       " 'end_game': True,\n",
       " 'player': (3, 4),\n",
       " 'monsters': [(3, 4), (4, 4)],\n",
       " 'diamonds': [(2, 3), (5, 3), (5, 4)],\n",
       " 'walls': [(1, 2),\n",
       "  (2, 1),\n",
       "  (2, 2),\n",
       "  (3, 3),\n",
       "  (4, 1),\n",
       "  (4, 2),\n",
       "  (4, 5),\n",
       "  (4, 6),\n",
       "  (6, 3),\n",
       "  (6, 6)],\n",
       " 'possible_actions': [1, 2, 5, 6, 7, 8, 9]}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'reward': 1,\n",
       " 'total_score': 1,\n",
       " 'end_game': False,\n",
       " 'player': (4, 0),\n",
       " 'monsters': [(1, 4), (6, 3)],\n",
       " 'diamonds': [(0, 7), (1, 5), (4, 5)],\n",
       " 'walls': [(1, 3),\n",
       "  (2, 0),\n",
       "  (2, 1),\n",
       "  (2, 5),\n",
       "  (3, 6),\n",
       "  (4, 1),\n",
       "  (4, 4),\n",
       "  (7, 2),\n",
       "  (7, 4),\n",
       "  (7, 6)],\n",
       " 'possible_actions': [2, 3, 5, 8, 9]}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.get_obs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Render random-action game\n",
    "obs = env.reset()\n",
    "while not obs['end_game']:\n",
    "    action = random.choice(obs['possible_actions'])\n",
    "    obs = env.make_action(action)\n",
    "    env.render()\n",
    "    time.sleep(0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean:  47.39 \n",
      "Median:  8.0\n"
     ]
    }
   ],
   "source": [
    "game_scores=[]\n",
    "for one_game in range(100):\n",
    "    obs = env.reset()\n",
    "    while not obs['end_game']:    \n",
    "        action = random.choice(obs['possible_actions'])\n",
    "        obs = env.make_action(action)\n",
    "    game_scores.append(obs['total_score'])\n",
    "print('Mean: ',np.mean(game_scores),'\\nMedian: ',np.median(game_scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DQN Mini-Pacman"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import gc\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "from keras.models import Sequential, clone_model\n",
    "from keras.layers import Dense, InputLayer\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import CSVLogger, TensorBoard\n",
    "import keras.backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dqn_model(input_shape, nb_actions):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(units=8, input_shape=input_shape, activation='relu'))\n",
    "    model.add(Dense(units=8, activation='relu'))\n",
    "    model.add(Dense(units=8, activation='relu'))\n",
    "    model.add(Dense(nb_actions, activation='linear'))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "action_to_dxdy = {1: (1, -1),\n",
    "                  2: (1, 0),\n",
    "                  3: (1, 1),\n",
    "                  4: (0, -1),\n",
    "                  5: (0, 0),\n",
    "                  6: (0, 1),\n",
    "                  7: (-1, -1),\n",
    "                  8: (-1, 0),\n",
    "                  9: (-1, 1)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_state(obs):\n",
    "    v = []\n",
    "    x,y = obs['player']\n",
    "    v.append(x)\n",
    "    v.append(y)\n",
    "    for x, y in obs['monsters']:\n",
    "        v.append(x)\n",
    "        v.append(y)\n",
    "    for x, y in obs['diamonds']:\n",
    "        v.append(x)\n",
    "        v.append(y)\n",
    "    for x, y in obs['walls']:\n",
    "        v.append(x)\n",
    "        v.append(y)\n",
    "    return v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QLearn:\n",
    "    def __init__(self, gamma=0.95, alpha=0.05):\n",
    "        from collections import defaultdict\n",
    "        self.gamma = gamma\n",
    "        self.alpha = alpha\n",
    "        self.qmap = defaultdict(int)\n",
    "\n",
    "    def iteration(self, old_state, old_action, reward, new_state, new_possible_actions):\n",
    "        # Produce iteration step (update Q-Value estimates)\n",
    "        old_stateaction = tuple(old_state) + (old_action,)\n",
    "        max_q = max([self.qmap[tuple(new_state) + (a,)] for a in new_possible_actions])\n",
    "        self.qmap[old_stateaction] = (1-self.alpha)*self.qmap[old_stateaction] + self.alpha*(reward+self.gamma*max_q)\n",
    "        return\n",
    "\n",
    "    def best_action(self, state, possible_actions):\n",
    "        # Get the action with highest Q-Value estimate for specific state\n",
    "        a, q = max([(a, self.qmap[tuple(state) + (a,)]) for a in possible_actions], key=lambda x: x[1])\n",
    "        return a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_shape:  (32,)\n",
      "nb_actions:  9\n",
      "WARNING:tensorflow:From /anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    }
   ],
   "source": [
    "input_shape = (len(get_state(env.reset())),)\n",
    "nb_actions = len(action_to_dxdy)\n",
    "print('input_shape: ',input_shape)\n",
    "print('nb_actions: ',nb_actions)\n",
    "\n",
    "online_network = create_dqn_model(input_shape, nb_actions)\n",
    "online_network.compile(optimizer=Adam(), loss='mse')\n",
    "target_network = clone_model(online_network)\n",
    "target_network.set_weights(online_network.get_weights())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 8)                 264       \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 8)                 72        \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 8)                 72        \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 9)                 81        \n",
      "=================================================================\n",
      "Total params: 489\n",
      "Trainable params: 489\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    },
    {
     "data": {
      "image/svg+xml": [
       "<svg height=\"337pt\" viewBox=\"0.00 0.00 112.25 337.00\" width=\"112pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g class=\"graph\" id=\"graph0\" transform=\"scale(1 1) rotate(0) translate(4 333)\">\n",
       "<title>G</title>\n",
       "<polygon fill=\"#ffffff\" points=\"-4,4 -4,-333 108.252,-333 108.252,4 -4,4\" stroke=\"transparent\"/>\n",
       "<!-- 48043958568 -->\n",
       "<g class=\"node\" id=\"node1\">\n",
       "<title>48043958568</title>\n",
       "<polygon fill=\"none\" points=\"0,-219.5 0,-255.5 104.252,-255.5 104.252,-219.5 0,-219.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"52.126\" y=\"-233.3\">dense_1: Dense</text>\n",
       "</g>\n",
       "<!-- 48043960192 -->\n",
       "<g class=\"node\" id=\"node2\">\n",
       "<title>48043960192</title>\n",
       "<polygon fill=\"none\" points=\"0,-146.5 0,-182.5 104.252,-182.5 104.252,-146.5 0,-146.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"52.126\" y=\"-160.3\">dense_2: Dense</text>\n",
       "</g>\n",
       "<!-- 48043958568&#45;&gt;48043960192 -->\n",
       "<g class=\"edge\" id=\"edge2\">\n",
       "<title>48043958568-&gt;48043960192</title>\n",
       "<path d=\"M52.126,-219.4551C52.126,-211.3828 52.126,-201.6764 52.126,-192.6817\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"55.6261,-192.5903 52.126,-182.5904 48.6261,-192.5904 55.6261,-192.5903\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 48044118928 -->\n",
       "<g class=\"node\" id=\"node3\">\n",
       "<title>48044118928</title>\n",
       "<polygon fill=\"none\" points=\"0,-73.5 0,-109.5 104.252,-109.5 104.252,-73.5 0,-73.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"52.126\" y=\"-87.3\">dense_3: Dense</text>\n",
       "</g>\n",
       "<!-- 48043960192&#45;&gt;48044118928 -->\n",
       "<g class=\"edge\" id=\"edge3\">\n",
       "<title>48043960192-&gt;48044118928</title>\n",
       "<path d=\"M52.126,-146.4551C52.126,-138.3828 52.126,-128.6764 52.126,-119.6817\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"55.6261,-119.5903 52.126,-109.5904 48.6261,-119.5904 55.6261,-119.5903\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 48044295280 -->\n",
       "<g class=\"node\" id=\"node4\">\n",
       "<title>48044295280</title>\n",
       "<polygon fill=\"none\" points=\"0,-.5 0,-36.5 104.252,-36.5 104.252,-.5 0,-.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"52.126\" y=\"-14.3\">dense_4: Dense</text>\n",
       "</g>\n",
       "<!-- 48044118928&#45;&gt;48044295280 -->\n",
       "<g class=\"edge\" id=\"edge4\">\n",
       "<title>48044118928-&gt;48044295280</title>\n",
       "<path d=\"M52.126,-73.4551C52.126,-65.3828 52.126,-55.6764 52.126,-46.6817\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"55.6261,-46.5903 52.126,-36.5904 48.6261,-46.5904 55.6261,-46.5903\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 48043959184 -->\n",
       "<g class=\"node\" id=\"node5\">\n",
       "<title>48043959184</title>\n",
       "<polygon fill=\"none\" points=\"5.626,-292.5 5.626,-328.5 98.626,-328.5 98.626,-292.5 5.626,-292.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"52.126\" y=\"-306.3\">48043959184</text>\n",
       "</g>\n",
       "<!-- 48043959184&#45;&gt;48043958568 -->\n",
       "<g class=\"edge\" id=\"edge1\">\n",
       "<title>48043959184-&gt;48043958568</title>\n",
       "<path d=\"M52.126,-292.4551C52.126,-284.3828 52.126,-274.6764 52.126,-265.6817\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"55.6261,-265.5903 52.126,-255.5904 48.6261,-265.5904 55.6261,-265.5903\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>"
      ],
      "text/plain": [
       "<IPython.core.display.SVG object>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import SVG\n",
    "from keras.utils.vis_utils import model_to_dot\n",
    "print(online_network.summary())\n",
    "\n",
    "SVG(model_to_dot(online_network).create(prog='dot', format='svg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import plot_model\n",
    "plot_model(online_network, to_file='online_network.png',show_shapes=True,show_layer_names=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "replay_memory_maxlen = 1_000_000\n",
    "replay_memory = deque([], maxlen=replay_memory_maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def epsilon_greedy(q_values, epsilon, n_outputs):\n",
    "    if random.random() < epsilon:\n",
    "        return random.randrange(n_outputs)  # random action\n",
    "    else:\n",
    "        return np.argmax(q_values)  # q-optimal action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_steps = 100_000 # number of times \n",
    "warmup = 1_000 # first iterations after random initiation before training starts\n",
    "training_interval = 4 # number of steps after which dqn is retrained\n",
    "copy_steps = 2_000 # number of steps after which weights of \n",
    "                   # online network copied into target network\n",
    "gamma = 0.99 # discount rate\n",
    "batch_size = 64 # size of batch from replay memory \n",
    "eps_max = 1.0 # parameters of decaying sequence of eps\n",
    "eps_min = 0.05\n",
    "eps_decay_steps = 50_000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Observtion:  [[7, 1, 0, 1, 3, 2, 4, 2, 4, 4, 5, 4, 0, 3, 1, 5, 1, 7, 2, 3, 2, 4, 2, 5, 3, 3, 3, 4, 3, 5, 7, 5]]\n",
      "Q-values [[-0.4955921  -0.7388129   0.2947756  -0.14595076  0.03176532 -0.05919297\n",
      "  -0.46131817  0.14764033 -0.21158531]]\n"
     ]
    }
   ],
   "source": [
    "step = 0\n",
    "iteration = 0\n",
    "done = True \n",
    "\n",
    "obs = env.reset()\n",
    "print('Observtion: ',[get_state(obs)])\n",
    "q_values = online_network.predict(np.array([get_state(obs)]))\n",
    "print('Q-values',q_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epsilon:  1.0 , Action:  8\n"
     ]
    }
   ],
   "source": [
    "epsilon = max(eps_min, eps_max - (eps_max-eps_min) * step/eps_decay_steps)\n",
    "action = epsilon_greedy(q_values, epsilon, nb_actions)\n",
    "print('Epsilon: ',epsilon, ', Action: ', action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "next_obs= env.make_action(action + 1)\n",
    "reward = next_obs['reward']\n",
    "done = next_obs['end_game']\n",
    "replay_memory.append((obs, action, reward, next_obs, done))\n",
    "obs = next_obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Record in replay memory\n",
      "Previous state:  {'reward': 0, 'total_score': 0, 'end_game': False, 'player': (7, 1), 'monsters': [(0, 1), (3, 2)], 'diamonds': [(4, 2), (4, 4), (5, 4)], 'walls': [(0, 3), (1, 5), (1, 7), (2, 3), (2, 4), (2, 5), (3, 3), (3, 4), (3, 5), (7, 5)], 'possible_actions': [4, 5, 6, 7, 8, 9]}\n",
      "Action:  8\n",
      "Reward:  1\n",
      "Next state:  {'reward': 1, 'total_score': 1, 'end_game': False, 'player': (6, 2), 'monsters': [(1, 0), (3, 1)], 'diamonds': [(4, 2), (4, 4), (5, 4)], 'walls': [(0, 3), (1, 5), (1, 7), (2, 3), (2, 4), (2, 5), (3, 3), (3, 4), (3, 5), (7, 5)], 'possible_actions': [1, 2, 3, 4, 5, 6, 7, 8, 9]}\n",
      "Game end:  False\n"
     ]
    }
   ],
   "source": [
    "print('Record in replay memory')\n",
    "print('Previous state: ',replay_memory[0][0])\n",
    "print('Action: ',replay_memory[0][1])\n",
    "print('Reward: ',replay_memory[0][2])\n",
    "print('Next state: ',replay_memory[0][3])\n",
    "print('Game end: ',replay_memory[0][4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "65"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "step = 0\n",
    "iteration = 0\n",
    "done = True\n",
    "warmup = 64\n",
    "\n",
    "for iter in range(warmup):\n",
    "    if done:\n",
    "        obs = env.reset()\n",
    "    q_values = online_network.predict(np.array([get_state(obs)]))\n",
    "    epsilon = max(eps_min, eps_max - (eps_max-eps_min) * step/eps_decay_steps)\n",
    "    action = epsilon_greedy(q_values, epsilon, nb_actions)\n",
    "    next_obs= env.make_action(action + 1)\n",
    "    reward = next_obs['reward']\n",
    "    done = next_obs['end_game']\n",
    "    replay_memory.append((obs, action, reward, next_obs, done))\n",
    "    obs = next_obs\n",
    "len(replay_memory)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([5, 6, 6, 5, 1, 5, 6, 7, 0, 8, 5, 2, 5, 4, 2, 5, 2, 0, 1, 0, 4, 3,\n",
       "       8, 8, 4, 5, 8, 6, 4, 2, 4, 4, 3, 5, 5, 4, 8, 4, 0, 2, 8, 3, 1, 6,\n",
       "       5, 2, 2, 3, 1, 6, 3, 8, 8, 4, 8, 6, 8, 4, 7, 3, 8, 1, 4, 6])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "minibatch = random.sample(replay_memory, batch_size)\n",
    "replay_state = np.array([get_state(x[0]) for x in minibatch])\n",
    "replay_action = np.array([x[1]for x in minibatch])\n",
    "replay_rewards = np.array([x[2] for x in minibatch])\n",
    "replay_next_state = np.array([get_state(x[3]) for x in minibatch])\n",
    "replay_done = np.array([x[4] for x in minibatch], dtype=int)\n",
    "replay_action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target prediction shape:  (64, 9)\n",
      "Example of predicted values:  [-0.50585145 -0.7315772   0.6279112   0.14420241  0.27508873 -0.10717981\n",
      " -0.8325552   0.4355373  -0.33174753]\n"
     ]
    }
   ],
   "source": [
    "target_predict = target_network.predict(replay_next_state)\n",
    "print('Target prediction shape: ', target_predict.shape)\n",
    "print('Example of predicted values: ',target_predict[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the max:  (64,)\n"
     ]
    }
   ],
   "source": [
    "print('Shape of the max: ',np.amax(target_predict,axis=1).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target predicted by online network\n",
      "[[-1.51706398e-01 -1.34126276e-01  9.56300646e-04  4.97048050e-02\n",
      "  -6.77769855e-02  7.68779032e-03 -1.00923419e-01  2.98611950e-02\n",
      "  -1.08146146e-01]\n",
      " [-7.47463703e-02 -2.50267506e-01  1.51786476e-01 -1.18295059e-01\n",
      "   1.84495836e-01 -1.66529357e-01  3.88406664e-02 -3.34581882e-02\n",
      "  -3.41103524e-02]\n",
      " [-6.99378252e-01 -6.27969384e-01  7.53068179e-03  2.33250722e-01\n",
      "  -3.03930551e-01  3.36765051e-02 -4.72657740e-01  1.30811572e-01\n",
      "  -4.89663899e-01]\n",
      " [-6.13682389e-01 -7.85284460e-01  5.18227339e-01  2.80459315e-01\n",
      "   1.49262309e-01  2.63910592e-02 -6.95352614e-01  4.10202861e-01\n",
      "  -4.04189676e-01]\n",
      " [-2.75650382e-01 -7.92139113e-01  7.96026587e-01 -3.94928813e-01\n",
      "   4.89898324e-01 -2.29447991e-01 -9.62268353e-01  3.40225458e-01\n",
      "   8.03752840e-02]]\n",
      "Update with values predicted by target network\n",
      "[[-1.51706398e-01 -1.34126276e-01  9.56300646e-04  4.97048050e-02\n",
      "  -6.77769855e-02  0.00000000e+00 -1.00923419e-01  2.98611950e-02\n",
      "  -1.08146146e-01]\n",
      " [-7.47463703e-02 -2.50267506e-01  1.51786476e-01 -1.18295059e-01\n",
      "   1.84495836e-01 -1.66529357e-01  1.18111861e+00 -3.34581882e-02\n",
      "  -3.41103524e-02]\n",
      " [-6.99378252e-01 -6.27969384e-01  7.53068179e-03  2.33250722e-01\n",
      "  -3.03930551e-01  3.36765051e-02  1.27145565e+00  1.30811572e-01\n",
      "  -4.89663899e-01]\n",
      " [-6.13682389e-01 -7.85284460e-01  5.18227339e-01  2.80459315e-01\n",
      "   1.49262309e-01  1.62163210e+00 -6.95352614e-01  4.10202861e-01\n",
      "  -4.04189676e-01]\n",
      " [-2.75650382e-01  1.80068147e+00  7.96026587e-01 -3.94928813e-01\n",
      "   4.89898324e-01 -2.29447991e-01 -9.62268353e-01  3.40225458e-01\n",
      "   8.03752840e-02]]\n",
      "Replay actions and target_for_action\n",
      "5 0.0\n",
      "6 1.1811186461150647\n",
      "6 1.2714556616544723\n",
      "5 1.6216320979595185\n",
      "1 1.80068141579628\n"
     ]
    }
   ],
   "source": [
    "target_for_action = replay_rewards + (1-replay_done) * gamma * \\\n",
    "                                    np.amax(target_predict, axis=1)\n",
    "target = online_network.predict(replay_state)  # targets coincide with predictions ...\n",
    "print('Target predicted by online network')\n",
    "print(target[:5])\n",
    "target[np.arange(batch_size), replay_action] = target_for_action\n",
    "print('Update with values predicted by target network')\n",
    "print(target[:5])\n",
    "print('Replay actions and target_for_action')\n",
    "for i in range(5):\n",
    "    print(replay_action[i],target_for_action[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "dqn_model = load_model('saved_dqn_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_dqn_strategy(obs):\n",
    "    q_values = online_network.predict(np.array([get_state(obs)]))[0]\n",
    "    action = epsilon_greedy(q_values, 0.05, nb_actions)\n",
    "    return action+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your average score is 22.68, median is 4.0, saved log to 'test_pacman_log.json'. Do not forget to upload it for submission!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "4.0"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from mini_pacman import test\n",
    "test(strategy=test_dqn_strategy, log_file='test_pacman_log.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your average score is 38.618, median is 5.0, saved log to 'test_pacman_log.json'. Do not forget to upload it for submission!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "5.0"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from mini_pacman import test\n",
    "from mini_pacman import random_strategy\n",
    "test(strategy=random_strategy, log_file='test_pacman_log.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your average score is 107.476, median is 100.0, saved log to 'test_pacman_log.json'. Do not forget to upload it for submission!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "100.0"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from mini_pacman import test\n",
    "from mini_pacman import naive_strategy\n",
    "test(strategy=naive_strategy, log_file='test_pacman_log.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
